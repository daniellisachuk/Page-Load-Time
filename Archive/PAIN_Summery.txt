INPUTS:
  # Flow from Traffic
  # Main domain names (not URLs) to monitor
        (not universal, per known websites, universal possible?)

System consists of 2 subsystems

  Sub A - MODEL LEARNING:

      makes model for each main domain(the one user asked for)
      discovers and clusters subdomains aassociated to specific websites.

      # Given that downloaded HTTP objects while rendering pages vary from visit to visit
      (e.g., because of caching, persistent connections, modification in the content, personalized content etc.),
      PAIN analyzes the order in which groups of support domains typically appear.

      # Some support domains may be missing in a visit,
        others may not be relevant for indicating the website performance

      #uses groups of support domains to build models

        ML - Sub Dom Learning:

          # When a client is observed opening a flow to the main domain,
            the domains of flows that follow shall be considered within
            the subdomain group

          # not all flows are truly linked because user may access multiple
            services at the same time or bg-software

          # single subdomain may be shared by multiple core domains,
            while a main domain itself may act as subdomain for another services

          # When a flow to a core domain is observed, PAIN opens an Observation Window (OW) of duration dT.

          # Domains of all flows observed in dT become part of S', the candidates for forming S(subdomain group).

          # The longer dT, the more information is collected, with chances of polluting S'

          # Avoids polluting Sc by pruning candidate support domains in S' based on the frequency
            the domain d appears in observation windows of main domain.

          # high number of sample OWs is beneficial to reduce noise

        ML - Sub Dom Learning:

          # The timeline of support flows reflects the speed at which a website is loaded

          # PAIN leverages this behavior to calculate a score for d∈S.

          # The score is higher for support domains appearing further away in time
            from the core domain

          # score(c, di) is high if di appears often later in time than other
            domains in the observations windows of c. Similarly,
            it is lower if di usually appear close in time to the core domain.

        ML - Sub Dom Learning:

        ` # After scoring, PAIN identifies groups of support domains.

          # sort di ∈ S in increasing order of score(c, di)
            and split the domains in n groups

          # G1 will contain those support domains that often appear
            the closest to the core domain
            flow, wheres Gn will have the support domains that often appear
            the furthest to the core domain.

          # n is a parameter to be investigated.


  Sub B - PAIN Index Computation:

      extracts the actual performance index using the models.

      # PAIN considers all support domain flows
        generated by the client within the OW, and accounts them to the
        corresponding group.

      # measure the time at which flows in each group are observed.

      # A visit to a group is considered complete when the last
        flow in the group is observed.

      # PAIN calculates the index Pi, equals to the time difference between
        the starting of the last flow in the group i and the starting time
        of the core domains

      # groups can be absent if none of its support domains is in OW.
        * The browser cache contains all the objects that are
          hosted on a particular domain
        * the browser already opened a persistent connection toward the target domains.

        In this case, we do not consider the sample.

      # The intuition is that the website performance is mainly driven by the ability of
        the browser to obtain objects to render the pages

      # By considering all visits from
        all clients to c, PAIN builds statistics on the performance faced by
        clients.

      # Due to the intrinsic noisiness of flow-level measurements,
        PAIN assumes relevance when multiple measures are aggregated to
        contrast different users, time periods or locations.

  # Support scalable processing using Apache Spark

# It learns models directly from traffic, without requiring human
  intervention or any information collected at the client-side.

# a supervised design would result in a system that would allow one to predict the actual values for
  objective metrics, e.g., estimating OnLoad and SpeedIndex from the traffic.

# PAIN is fully able to pinpoint variations in objective metrics despite not being
  able to estimate their absolute values.

DATASETS:

  Synthetic

    # WebPageTest emulates networks based on DummyNet, a network emulation tool.

    # Given a list of URLs, it automatically navigates through each page while saving detailed statistics.

    # WebPageTest exports the HTTP Archive (HAR) for each page visit.

    two datasets to validate PAIN, namely SynthTypical and SynthDegraded,
    with respectively typical and degraded network conditions.

    SynthTypical:

      # built by letting WebPageTest visit 10 popular domains in Italy. For each domain,
        WebPageTest visits the homepage and 9 internal pages for a total of 100 pages.

      # We visit each page twice for each setup:
          * With empty browser cache
          * few seconds later for profiting from caching.
        The traffic is expected to vary strongly, since many objects are cached in the second case,
        complicating the identification of subdomains.

      # In total, WebPageTest recorded 6400 visits while building this first dataset (~48H)

    SynthDegraded

      # represents artificial conditions, in which we enforce link delay or bandwidth limits.

      # simulate scenarios in which website performance decreases due to worsening network conditions.
        * Adding from 100 ms to 500 ms extra per-packet delay
        * imposing a limit from 2.5 Mbit/s to 312.5 kbit/s on uplink and downlink access bandwidth.

      # we visit each page twice (cold and warm cache) and with 4 browsers.\

      # WebPageTest has performed 8000 visits for building this second dataset (~60H)


  ISP:

    # flow summaries exported by Tstat in a real deployment.

    # Tstat associates flow records to domain names requested by clients using the
      SNI information from TLS handshakes and by exploiting DNS traffic also observed
      in the network

    # PAIN isolates flows per ADSL installation and use them as the per-client
      traces. Each trace includes information about traffic of all users’
      devices connected at home.

    # 15 billion flows related to around 100,000 websites.


VALIDATION

  # support domains are often contacted after the OnLoad event has fired,
    e.g., due to browser pre-fetching or the presence of analytics scripts programmed to
    run after the page is loaded.

  # www.ebay.it : More than 40% of connections are issued after the
    browser completed loading the page.

  # For each website, we compute the Jaccard index similarity coefficient for the sets
    of support domains contacted for each pair of sub-pages.

  # If the Jaccard index is equal to one, the sets of support domains are equal.

  # Overall, varying the device used for accessing the web page does not affect
    the contacted support domains.

  # vary each parameter while comparing the PAIN index to the metrics exposed by our testbed,
    i.e., onLoad and SpeedIndex.

  # Only the correlations between objective metrics and the 3rd group of support flows
    (i.e., P3) are shown to improve visualization.

  # PAIN achieves high correlation coefficients when dT increases. When dT value is larger than 30-s,
    results do not improve further.

  # PAIN is not very sensitive to dT.

  # PAIN is not very sensitive to n either

  # Using a small value of n provides poor information.

  # the best correlation is achieved with earlier groups (i.e., smaller i) for the SpeedIndex,
    in particular for large n.

  # SpeedIndex is computed based on the visual progress of the page, which is
    usually achieved earlier than the onLoad event.

  # n = 4 seems to be a good trade-off for both metrics.(onLoad & SpeedTest)

  # results show that Pi reflects the network conditions, allowing ISPs to track
    degradation on the network that impacts website performance.

  # results reinforce that the metrics are correlated, and they vary according to the network conditions similarly.